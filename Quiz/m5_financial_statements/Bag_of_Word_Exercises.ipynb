{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table_of_content\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization\n",
    "\n",
    "First we read in our 10-k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 10-Ks item 1A for CIK = 0001065088 ...\n",
      "skipping EBAY_10k_2017.txt\n",
      "skipping EBAY_10k_2016.txt\n",
      "skipping EBAY_10k_2015.txt\n",
      "skipping EBAY_10k_2014.txt\n",
      "skipping EBAY_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0000320193 ...\n",
      "skipping AAPL_10k_2017.txt\n",
      "skipping AAPL_10k_2016.txt\n",
      "skipping AAPL_10k_2015.txt\n",
      "skipping AAPL_10k_2014.txt\n",
      "skipping AAPL_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0001310067 ...\n",
      "skipping SHLDQ_10k_2017.txt\n",
      "skipping SHLDQ_10k_2016.txt\n",
      "skipping SHLDQ_10k_2015.txt\n",
      "skipping SHLDQ_10k_2014.txt\n",
      "skipping SHLDQ_10k_2013.txt\n",
      "10-k files:  ['AAPL_10k_2013.txt', 'AAPL_10k_2014.txt', 'AAPL_10k_2015.txt', 'AAPL_10k_2016.txt', 'AAPL_10k_2017.txt', 'EBAY_10k_2013.txt', 'EBAY_10k_2014.txt', 'EBAY_10k_2015.txt', 'EBAY_10k_2016.txt', 'EBAY_10k_2017.txt', 'SHLDQ_10k_2013.txt', 'SHLDQ_10k_2014.txt', 'SHLDQ_10k_2015.txt', 'SHLDQ_10k_2016.txt', 'SHLDQ_10k_2017.txt']\n"
     ]
    }
   ],
   "source": [
    "# download some excerpts from 10-K files\n",
    "from download10k import get_files\n",
    "\n",
    "CIK = {'ebay': '0001065088', 'apple':'0000320193', 'sears': '0001310067'}\n",
    "get_files(CIK['ebay'], 'EBAY')\n",
    "get_files(CIK['apple'], 'AAPL')\n",
    "get_files(CIK['sears'], 'SHLDQ')\n",
    "\n",
    "\n",
    "# get a list of all 10-ks in our directory\n",
    "files=! ls *10k*.txt\n",
    "print(\"10-k files: \",files)\n",
    "files = [open(f,\"r\").read() for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we define useful functions to tokenize the texts into words, remove stop-words, and lemmatize and stem our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for nice number printing\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# tokenize and clean the text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# tokenize anything that is not a number and not a symbol\n",
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# get our list of stop_words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# add some extra stopwords\n",
    "stop_words |= {\"may\", \"business\", \"company\", \"could\", \"service\", \"result\", \"product\", \n",
    "               \"operation\", \"include\", \"law\", \"tax\", \"change\", \"financial\", \"require\",\n",
    "               \"cost\", \"market\", \"also\", \"user\", \"plan\", \"actual\", \"cash\", \"other\",\n",
    "               \"thereto\", \"thereof\", \"therefore\"}\n",
    "\n",
    "# useful function to print a dictionary sorted by value (largest first by default)\n",
    "def print_sorted(d, ascending=False):\n",
    "    factor = 1 if ascending else -1\n",
    "    sorted_list = sorted(d.items(), key=lambda v: factor*v[1])\n",
    "    for i, v in sorted_list:\n",
    "        print(\"{}: {:.3f}\".format(i, v))\n",
    "\n",
    "# convert text into bag-of-words\n",
    "def clean_text(txt):\n",
    "    lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),'n'),'v') \\\n",
    "                for w in word_tokenizer.tokenize(txt) if \\\n",
    "                w.isalpha() and w not in stop_words ]\n",
    "    return [ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) > 2 ]\n",
    "\n",
    "corpus = [clean_text(f) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bag_of_words\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that converts a list of tokenized words into bag-of-words, i.e. a dictionary that outputs the frequency count of a word\n",
    "\n",
    "** python already provide the `collections.Counter` class to perform this, but I encourage you to implement your own function as an exercise\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bag_of_words(words):\n",
    "    bag_of_words = {}\n",
    "    \n",
    "    for word in words:\n",
    "        bag_of_words[word] = bag_of_words.get(word, 0) + 1\n",
    "    \n",
    "    return bag_of_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentiment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiments\n",
    "Count the fraction of words that appear in a wordlist, returning a sentiment score between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\textrm{score} = \\frac{\\textrm{Number of words in document matching wordlist}}{\\textrm{Number of words in document}}\n",
    "$$\n",
    "\n",
    "Implement the score in a function `get_sentiment(words, wordlist)`, where words is a list of words. Feel free to use the previous `bag_of_words` function. \n",
    "(*for extra challenge, try to code the function in one-line*)\n",
    "\n",
    "Here, I've included a positive and negative wordlist that I constructed by hand. Due to copyright issues, we are not able to provide other commonly used wordlists. I encourage you to try out different wordlists on your own.\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive words:  {'celebr', 'meritori', 'master', 'extraordinarili', 'growth', 'fantast', 'glow', 'monument', 'attract', 'rich', 'amelior', 'better', 'stabil', 'wholesom', 'clear', 'independ', 'enchant', 'jubil', 'recoveri', 'superb', 'progress', 'win', 'impress', 'beauti', 'optimist', 'flourish', 'stupend', 'heal', 'lead', 'terrif', 'fresh', 'higher', 'endors', 'creativ', 'championship', 'brilliant', 'lucrat', 'loyal', 'revolutionari', 'tremend', 'admir', 'origin', 'nobli', 'ingeni', 'resourc', 'groundbreak', 'bolster', 'gain', 'virtuous', 'resolv', 'miracul', 'revolut', 'benefit', 'benefici', 'exuber', 'dynam', 'approv', 'advanc', 'fabul', 'good', 'surpass', 'fortun', 'distinguish', 'recov', 'promin', 'exemplifi', 'nourish', 'surmount', 'compliment', 'great', 'accept', 'magic', 'profici', 'nurtur', 'legendari', 'trendi', 'exquisit', 'command', 'clean', 'talent', 'safe', 'effici', 'best', 'boon', 'eleg', 'restor', 'distinct', 'respect', 'merit', 'abund', 'honor', 'excel', 'adorn', 'loyalti', 'surg', 'reliev', 'boost', 'bless', 'effect', 'wealth', 'dazzl', 'extraordinari', 'fascin', 'posit', 'glamor', 'accomplish', 'pleasant', 'favor', 'appreci', 'reinforc', 'astonish', 'nobl', 'revolution', 'gift', 'opportun', 'upbeat', 'amaz', 'ador', 'improv', 'outperform', 'satisfi', 'optim', 'solut', 'appeal', 'plenti', 'esteem', 'happili', 'strength', 'transform', 'earnest', 'happi', 'courag', 'passion', 'booster', 'satisfactori', 'visionari', 'delight', 'complet', 'rebound', 'cheer', 'forefront', 'enhanc', 'perfect', 'prodigi', 'enrich', 'elev', 'solv', 'aptitud', 'prestigi', 'award', 'benign', 'innov', 'outstand', 'polish', 'famous', 'prestig', 'wealthi', 'applaus', 'vibrant', 'incred', 'strong', 'support', 'victori', 'divin', 'trendili', 'high', 'belov', 'persever', 'enthusiasm', 'success', 'exhilar', 'encourag', 'wow', 'nobil', 'awesom', 'power', 'prais', 'harmoni', 'spring', 'wonder', 'laud', 'special', 'bright', 'prosper', 'invent', 'congratul', 'benevol', 'generous', 'miracl', 'product', 'flawless', 'insight', 'acclaim', 'champion', 'paradis', 'reward', 'stabl', 'marvel', 'winner', 'except', 'relief', 'brighter', 'achiev', 'buoy', 'devot', 'love', 'actual', 'refresh', 'bliss', 'inspir', 'upstand', 'popular', 'attain', 'motiv', 'succeed'}\n",
      "negative words:  {'suspend', 'calam', 'diminish', 'foul', 'terribl', 'fruitless', 'negat', 'shutdown', 'unneed', 'unfruit', 'disapprov', 'deni', 'attrit', 'taint', 'lack', 'outlaw', 'danger', 'debt', 'argument', 'overpay', 'delay', 'storm', 'fail', 'doomsday', 'rot', 'indict', 'complic', 'harsh', 'crime', 'arrest', 'sacrifici', 'postpon', 'instabl', 'lazili', 'anomali', 'blame', 'empti', 'unnecessarili', 'closur', 'disqualif', 'imped', 'inabl', 'trivial', 'harsher', 'offens', 'horrid', 'unstabl', 'malfunct', 'insolv', 'unattain', 'wors', 'perjuri', 'hardship', 'break', 'aggrav', 'counterfeit', 'discredit', 'untal', 'blackmail', 'steal', 'argu', 'impot', 'embezzl', 'woe', 'weak', 'trial', 'excess', 'ineffect', 'conced', 'frustrat', 'unfair', 'powerless', 'disregard', 'neglect', 'unfavor', 'deplor', 'atroci', 'deris', 'desol', 'misappli', 'recal', 'misus', 'unscrupul', 'critic', 'pollut', 'default', 'error', 'rotten', 'disfavor', 'devast', 'disclaim', 'doubt', 'plea', 'dark', 'cataclysm', 'wrong', 'decept', 'inconveni', 'uncontrol', 'lie', 'unauthor', 'difficulti', 'penalti', 'delus', 'disadvantag', 'nullifi', 'appal', 'renounc', 'guilti', 'indign', 'null', 'miseri', 'attenu', 'repugn', 'defianc', 'troublesom', 'troubl', 'invalid', 'fals', 'slow', 'weaken', 'desist', 'complaint', 'unnecessari', 'retreat', 'injuri', 'advers', 'avoid', 'breach', 'temper', 'bankruptci', 'unabl', 'vile', 'catastroph', 'stress', 'corrupt', 'apathet', 'exhaust', 'hardest', 'disabl', 'scandal', 'boycott', 'unfortun', 'devalu', 'flaw', 'hamper', 'death', 'imperil', 'abandon', 'lazi', 'unpopular', 'cloudi', 'breakabl', 'deter', 'vilifi', 'ineffici', 'underwhelm', 'sacrific', 'erron', 'murder', 'blacklist', 'scourg', 'nastili', 'incap', 'agoni', 'dampen', 'loot', 'unsettl', 'uninspir', 'unproduct', 'lag', 'uneth', 'termin', 'exacerb', 'contempt', 'sue', 'underperform', 'dirtili', 'convict', 'illeg', 'worsen', 'peril', 'untrustworthi', 'bankrupt', 'crash', 'blow', 'sever', 'incident', 'afflict', 'impedi', 'balk', 'litig', 'confus', 'accident', 'collud', 'refus', 'unwelcom', 'strain', 'close', 'gloom', 'incompat', 'burden', 'suspici', 'dull', 'hate', 'conflict', 'disinterest', 'imposs', 'fraud', 'nefari', 'guilt', 'shadi', 'dirti', 'usurp', 'hurt', 'harm', 'fire', 'upset', 'difficult', 'wreck', 'dysfunct', 'suspens', 'nasti', 'halt', 'shrink', 'desper', 'ugli', 'falsifi', 'strike', 'malic', 'uneffect', 'disturb', 'pessim', 'distress', 'violent', 'fraudul', 'escal', 'sorrow', 'reject', 'violat', 'unlaw', 'urgent', 'ceas', 'accid', 'unhappi', 'obnoxi', 'dissolv', 'cutback', 'objection', 'obsolesc', 'harder', 'deplet', 'failur', 'illicit', 'lower', 'aw', 'abhorr', 'bust', 'apathi', 'pessimist', 'fear', 'threat', 'erod', 'damag', 'fault', 'curs', 'pressur', 'tragedi', 'depress', 'thieveri', 'lawsuit', 'scorn', 'forfeit', 'horribl', 'vice', 'defiant', 'eros', 'bribe', 'hinder', 'misappl', 'destroy', 'crack', 'illiquid', 'terror', 'condemn', 'intimid', 'collus', 'notori', 'handicap', 'faulti', 'jail', 'fright', 'disast', 'insan', 'thiev', 'deterior', 'hard', 'ban', 'object', 'turbul', 'inviabl', 'mockeri', 'insol', 'drain', 'nuisanc', 'evil', 'malign', 'forgeri', 'antiqu', 'undermin', 'odious', 'hatr', 'hazard', 'impoverish', 'panic', 'hostil', 'tragic', 'oppress', 'gruesom', 'bottleneck', 'horrend', 'wick', 'disqualifi', 'contradict', 'shame', 'repudi', 'dead', 'injur', 'repuls', 'declin', 'discourag', 'subpoena', 'fiasco', 'impeach', 'horror', 'inferior', 'deprav', 'crisi', 'distast', 'downgrad', 'dismal', 'wast', 'content', 'worri', 'demot', 'abhor', 'ridicul', 'leak', 'abnorm', 'obsolet', 'inept', 'worst', 'doom', 'headach', 'incid', 'investig', 'misfortun', 'fatal', 'revolt', 'worrisom', 'gloomi', 'degrad', 'vulgar', 'disgust', 'harrow', 'obstruct', 'ruin', 'accus', 'dread', 'manipul', 'unattract', 'low', 'abomin', 'plung', 'exploit', 'unworthi', 'weari', 'kill'}\n"
     ]
    }
   ],
   "source": [
    "# load wordlist first\n",
    "import pickle\n",
    "\n",
    "with open('positive_words.pickle', 'rb') as f:\n",
    "    positive_words = pickle.load(f)\n",
    "    # also need to stem and lemmatize the text\n",
    "    positive_words = set(clean_text(\" \".join(positive_words)))\n",
    "    \n",
    "with open('negative_words.pickle', 'rb') as f:\n",
    "    negative_words = pickle.load(f)\n",
    "    negative_words = set(clean_text(\" \".join(negative_words)))\n",
    "    \n",
    "# check out the list\n",
    "print(\"positive words: \", positive_words)\n",
    "print(\"negative words: \", negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(txt, wordlist):\n",
    "    return len([word for word in txt if word in wordlist]) / len(bag_of_words(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.132  0.132  0.131  0.133  0.129  0.321  0.335  0.275  0.183  0.189\n",
      "  0.093  0.092  0.123  0.139  0.163]\n",
      "[ 0.211  0.213  0.213  0.217  0.22   0.48   0.508  0.432  0.355  0.366\n",
      "  0.15   0.162  0.192  0.192  0.24 ]\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "positive_sentiments = np.array([ get_sentiment(c, positive_words) for c in corpus ])\n",
    "print(positive_sentiments)\n",
    "\n",
    "negative_sentiments = np.array([ get_sentiment(c, negative_words) for c in corpus ])\n",
    "print(negative_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**before continuing part 2, go through the lesson material first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute_idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Document Vector Exercises\n",
    "\n",
    "## 3. Computer idf\n",
    "Given a corpus, or a list of bag-of-words, we want to compute for each word $w$, the inverse-document-frequency, or ${\\rm idf}(w)$. This can be done in a few steps:\n",
    "\n",
    "1. Gather a set of all the words in all the bag-of-words (python set comes in handy, and the union operator `|` is useful here)\n",
    "\n",
    "\n",
    "2. Loop over each word $w$, and compute ${\\rm df}_w$, the number of documents where this word appears at least once. A dictionary is useful for keeping track of ${\\rm df}_w$\n",
    "\n",
    "\n",
    "3. After computing ${\\rm df}_w$, we can compute ${\\rm idf}(w)$. There are usually two possibilities, the simplest one is \n",
    "$${\\rm idf}(w)=\\frac{N}{{\\rm df}_w}$$\n",
    "Where $N$ is the total number of documents in the corpus and $df_w$ the number of documents that contain the word $w$. Frequently, a logarithm term is added as well\n",
    "$${\\rm idf}(w)=\\log\\frac{N}{{\\rm df}_w}$$\n",
    "One issue with using the logarithm is that when ${\\rm df}_w = N$, ${\\rm idf}(w)=0$, indicating that words common to all documents would be ignored. If we don't want this behavior, we can define ${\\rm idf}(w)=\\log\\left(1+N/{\\rm df}_w\\right)$ or ${\\rm idf}(w)=1+\\log\\left(N/{\\rm df}_w\\right)$ instead. For us, we'll not use the extra +1 for ${\\rm idf}$.\n",
    "\n",
    "In the following, define a function called `get_idf(corpus, include_log=True)` that computes ${\\rm idf}(w)$ for all the words in a corpus, where `corpus` for us is a processed list of bag-of-words (stemmed and lemmatized). The optional parameter `include_log` includes the logarithm in the computation.\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute idf\n",
    "def get_idf(corpus, include_log=True):\n",
    "    N = len(corpus)\n",
    "    freq = defaultdict(int)\n",
    "    words = set()\n",
    "    for c in corpus:\n",
    "        words |= set(c)\n",
    "        \n",
    "    for w in words:\n",
    "        freq[w] = sum([ w in c for c in corpus])\n",
    "\n",
    "    if include_log:\n",
    "        return { w: np.log(N/freq[w]) for w in freq }\n",
    "    else:\n",
    "        return { w:N/freq[w] for w in freq }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see many idf values = 0! This is by design, because we have ${\\rm idf}(w)=\\log N_d/{\\rm df}_w$ and $N_d/{\\rm df}_w=1$ for the most common words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test your code\n",
    "idf=get_idf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute_tf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute tf\n",
    "\n",
    "Below we will compute ${\\rm tf}(w,d)$, or the term frequency for a given word $w$, in a given document $d$. Since our ultimate goal is to compute a document vector, we'd like to keep a few things in mind\n",
    "\n",
    "1. Store the ${\\rm tf}(w, d)$ for each word in a document as a dictionary\n",
    "2. Even when a word doesn't appear in the document $d$, we still want to keep a $0$ entry in the dictionary. This is important when we convert the dictionary to a vector, where zero entries are important\n",
    "\n",
    "\n",
    "There are multiple definitions for ${\\rm tf}(w,d)$, the simplest one is\n",
    "\n",
    "$$\n",
    "{\\rm tf}(w,d)=\\frac{f_{w,d}}{a_d}\n",
    "$$\n",
    "\n",
    "where $f_{w,d}$ is the number of occurence of the word $w$ in the document $d$, and $a$ the average occurence of all the words in that document for normalization. Just like ${\\rm idf}(w)$, a logarithm can be added\n",
    "\n",
    "$$\n",
    "{\\rm tf}(w,d)=\\begin{cases}\n",
    "\\frac{1+\\log f_{w,d}}{1+\\log a_d} & f_{w,d} > 0 \\\\\n",
    "0 & f_{w,d} = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Implement the function `get_df(txt, include_log=True)` that computes ${\\rm tf}(w,d)$ for each word in the document (returns a defaultdict(int), so that when supplying words not in the document the dictionary will yield zero instead of an error). Also include the optional parameter `include_log` that enables the additional logarithm term in the computation. I suggest also adding a function called `_tf` that computes the function above. \n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "def _tf(freq, avg, include_log=True):\n",
    "    if include_log:\n",
    "        return (1 + np.log(freq)) / (1 + np.log(avg))\n",
    "    else:\n",
    "        return freq / avg\n",
    "    \n",
    "    \n",
    "\n",
    "def get_tf(txt, include_log=True):\n",
    "    tf = defaultdict(int)\n",
    "    \n",
    "    for word in txt:\n",
    "        tf[word] += 1\n",
    "        \n",
    "    avg = len(tf) / len(txt)\n",
    "    \n",
    "    for key, value in tf.items():\n",
    "        tf[key] = _tf(value, avg, include_log)\n",
    "    \n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [ get_tf(c) for c in corpus ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"doc_vector\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Vector\n",
    "Combine the implementation for ${\\rm tf}(w,d)$ and ${\\rm idf}(w)$ to compute a word-vector for each document in a corpus. Don't forget the zero-padding that is needed when a word appears in some document but not others. \n",
    "\n",
    "Implmenet the function `get_vectors(tf,idf)`, the input is the output computed by `get_tf` and `get_idf`\n",
    "\n",
    "(*optional challenge: implement in one line!*)\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(tf, idf):\n",
    "    return np.array([ tf[w]*idf[w] for w in idf ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.     0.     0.    ...,  0.    -2.752  0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -2.589  0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -2.301  0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -2.264  0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -2.273  0.   ]\n",
      "[ 0.    -3.694 -2.748 ..., -1.065 -0.998  0.   ]\n",
      "[ 0.     0.    -2.569 ..., -0.996 -0.956 -2.04 ]\n",
      "[ 0.     0.     0.    ..., -1.22  -1.041  0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -1.344  0.   ]\n",
      "[-3.425  0.     0.    ...,  0.    -1.295  0.   ]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.     0.     0.    ..., -5.064  0.     0.   ]\n",
      "[ 0.     0.     0.    ...,  0.    -1.588  0.   ]\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "doc_vectors = [ get_vector(tf, idf) for tf in tfs ]\n",
    "\n",
    "for v in doc_vectors:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"similarity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Similarity\n",
    "\n",
    "Given two word-vectors $\\vec u$ (or $u_i$) and $\\vec v$ (or $v_i$), corresponding to two documents, we want to compute different similarity metrics. \n",
    "\n",
    "1. Cosine similarity, defined by \n",
    "$$\n",
    "{\\rm Sim}_{\\cos} = \\frac{\\vec u \\cdot \\vec v}{|\\vec u| |\\vec v|}\n",
    "$$\n",
    "\n",
    "2. Jaccard similarity, defined by\n",
    "$$\n",
    "{\\rm Sim}_{\\rm Jaccard} = \\frac{\\sum_i \\min\\{u_i, v_i\\}}{\\sum_i \\max\\{u_i, v_i\\}}\n",
    "$$\n",
    "\n",
    "Implement the function similarity as `sim_cis(u,v)` and `sim_jac(u,v)`. Utilize the numpy functions `numpy.linalg.norm` to compute norm of a vector and `np.dot` for computing dot-products. `np.minimum` and `np.maximum` are also useful vectorized pair-wise minimum and maximum functions.\n",
    "\n",
    "(*optional challenge: implement both functions in one line!*)\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def sim_cos(u,v):\n",
    "    return np.dot(u,v)/(norm(u)*norm(v))\n",
    "\n",
    "def sim_jac(u,v):\n",
    "    return np.sum(np.minimum(u,v))/np.sum(np.maximum(u,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:\n",
      "[[ 1.     0.95   0.912  0.903  0.89   0.122  0.12   0.136  0.135  0.139\n",
      "  -0.017  0.025  0.03   0.041  0.074]\n",
      " [ 0.95   1.     0.961  0.947  0.932  0.116  0.114  0.132  0.127  0.13\n",
      "  -0.017  0.024  0.03   0.041  0.071]\n",
      " [ 0.912  0.961  1.     0.986  0.931  0.122  0.121  0.139  0.133  0.137\n",
      "  -0.017  0.024  0.032  0.041  0.074]\n",
      " [ 0.903  0.947  0.986  1.     0.941  0.124  0.123  0.138  0.135  0.141\n",
      "  -0.018  0.025  0.033  0.043  0.076]\n",
      " [ 0.89   0.932  0.931  0.941  1.     0.121  0.12   0.136  0.134  0.137\n",
      "  -0.021  0.027  0.035  0.044  0.073]\n",
      " [ 0.122  0.116  0.122  0.124  0.121  1.     0.893  0.591  0.469  0.473\n",
      "  -0.061  0.078  0.115  0.126  0.145]\n",
      " [ 0.12   0.114  0.121  0.123  0.12   0.893  1.     0.601  0.452  0.455\n",
      "  -0.059  0.072  0.107  0.119  0.142]\n",
      " [ 0.136  0.132  0.139  0.138  0.136  0.591  0.601  1.     0.654  0.661\n",
      "  -0.069  0.083  0.116  0.126  0.147]\n",
      " [ 0.135  0.127  0.133  0.135  0.134  0.469  0.452  0.654  1.     0.954\n",
      "  -0.071  0.084  0.095  0.101  0.133]\n",
      " [ 0.139  0.13   0.137  0.141  0.137  0.473  0.455  0.661  0.954  1.    -0.074\n",
      "   0.087  0.098  0.102  0.135]\n",
      " [-0.017 -0.017 -0.017 -0.018 -0.021 -0.061 -0.059 -0.069 -0.071 -0.074  1.\n",
      "  -0.843 -0.558 -0.435 -0.354]\n",
      " [ 0.025  0.024  0.024  0.025  0.027  0.078  0.072  0.083  0.084  0.087\n",
      "  -0.843  1.     0.602  0.482  0.387]\n",
      " [ 0.03   0.03   0.032  0.033  0.035  0.115  0.107  0.116  0.095  0.098\n",
      "  -0.558  0.602  1.     0.719  0.507]\n",
      " [ 0.041  0.041  0.041  0.043  0.044  0.126  0.119  0.126  0.101  0.102\n",
      "  -0.435  0.482  0.719  1.     0.672]\n",
      " [ 0.074  0.071  0.074  0.076  0.073  0.145  0.142  0.147  0.133  0.135\n",
      "  -0.354  0.387  0.507  0.672  1.   ]]\n",
      "\n",
      "Jaccard Similarity:\n",
      "[[   1.       1.156    1.272    1.303    1.332   12.575   13.046   10.083\n",
      "     8.89     8.799   -0.586  310.838   25.787   20.232   13.423]\n",
      " [   1.156    1.       1.126    1.159    1.182   12.444   12.938    9.806\n",
      "     8.969    8.878   -0.576  319.706   25.954   20.27    13.357]\n",
      " [   1.272    1.126    1.       1.037    1.13    11.769   12.146    9.254\n",
      "     8.538    8.441   -0.577  320.423   25.191   19.953   12.812]\n",
      " [   1.303    1.159    1.037    1.       1.109   11.628   12.012    9.213\n",
      "     8.421    8.267   -0.566  321.236   24.841   19.773   12.688]\n",
      " [   1.332    1.182    1.13     1.109    1.      11.773   12.162    9.216\n",
      "     8.407    8.32    -0.567  302.175   24.354   19.269   12.806]\n",
      " [  12.575   12.444   11.769   11.628   11.773    1.       1.283    2.361\n",
      "     3.324    3.326   -1.634  138.956   17.251   13.454    9.605]\n",
      " [  13.046   12.938   12.146   12.012   12.162    1.283    1.       2.353\n",
      "     3.491    3.478   -1.708  146.062   18.571   14.318    9.918]\n",
      " [  10.083    9.806    9.254    9.213    9.216    2.361    2.353    1.\n",
      "     2.129    2.101   -1.107  144.865   16.204   12.535    8.813]\n",
      " [   8.89     8.969    8.538    8.421    8.407    3.324    3.491    2.129\n",
      "     1.       1.103   -0.905  141.204   15.812   12.495    8.746]\n",
      " [   8.799    8.878    8.441    8.267    8.32     3.326    3.478    2.101\n",
      "     1.103    1.      -0.883  141.884   15.764   12.51     8.699]\n",
      " [  -0.586   -0.576   -0.577   -0.566   -0.567   -1.634   -1.708   -1.107\n",
      "    -0.905   -0.883    1.      -9.729   -1.099   -1.133   -1.07 ]\n",
      " [ 310.838  319.706  320.423  321.236  302.175  138.956  146.062  144.865\n",
      "   141.204  141.884   -9.729    1.      16.669   21.934   32.455]\n",
      " [  25.787   25.954   25.191   24.841   24.354   17.251   18.571   16.204\n",
      "    15.812   15.764   -1.099   16.669    1.       1.877    3.242]\n",
      " [  20.232   20.27    19.953   19.773   19.269   13.454   14.318   12.535\n",
      "    12.495   12.51    -1.133   21.934    1.877    1.       2.229]\n",
      " [  13.423   13.357   12.812   12.688   12.806    9.605    9.918    8.813\n",
      "     8.746    8.699   -1.07    32.455    3.242    2.229    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# test your co\n",
    "# compute all the pairwise similarity metrics\n",
    "size = len(doc_vectors)\n",
    "matrix_cos = np.zeros((size,size))\n",
    "matrix_jac = np.zeros((size,size))\n",
    "\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        u = doc_vectors[i]\n",
    "        v = doc_vectors[j]\n",
    "        matrix_cos[i][j] = sim_cos(u,v)\n",
    "        matrix_jac[i][j] = sim_jac(u,v)\n",
    "        \n",
    "print(\"Cosine Similarity:\")\n",
    "print(matrix_cos)\n",
    "\n",
    "print()\n",
    "print(\"Jaccard Similarity:\")\n",
    "print(matrix_jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Job! You've finished all the exercises!\n",
    "\n",
    "Here is an optional bonus challenge. We often need to debug other people's code to figure out what's wrong. It's particularly difficult when the code doesn't give errors but computes the wrong quantity. Can you describe why the following implementations for some of the exercises above may be wrong? Highlight the words at the bottom to reveal the solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_wrong(corpus, include_log=True):\n",
    "    freq = defaultdict(int)\n",
    "    for c in corpus:\n",
    "        for w in c:\n",
    "            freq[w] += 1\n",
    "        \n",
    "    N = len(corpus)\n",
    "    if include_log:\n",
    "        return { w:log(N/freq[w]) for w in freq }\n",
    "    else:\n",
    "        return { w:N/freq[w] for w in freq }\n",
    "\n",
    "\n",
    "def get_sentiment_wrong(txt, wordlist):\n",
    "    matching_words = [ w for w in wordlist if w in txt ]\n",
    "    return len(matching_words)/len(txt)\n",
    "\n",
    "def get_vectors_wrong(tf, idf):\n",
    "    return np.array([ tf[w]*idf[w] for w in tf ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">\n",
    "Solution\n",
    "\n",
    "get_idf: the defaultdict freq here computes the total number of occurences in all the document. We only want to count it once when a word appears in a document. This may lead to a document frequency larger than N, leading to a negative idf! \n",
    "\n",
    "get_sentiment_wrong: if a word w appears twice in the document, it won't be counted properly!\n",
    "\n",
    "get_vectors_wrong: tf may not contain all the words in idf. We need zero padding for words that appear in idf but not in tf! \n",
    "</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
